#!/bin/bash

# Ù‚Ø¯Ù… 1: Ø±ÙØ¹ Ù…Ø´Ú©Ù„ snscrape Ø¨Ø§ Ù¾Ø§ÛŒØªÙˆÙ† 3.12
echo "ðŸ”§ Ø±ÙØ¹ Ù…Ø´Ú©Ù„ snscrape Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØªÙˆÙ† 3.12..."
pip uninstall -y snscrape > /dev/null 2>&1

# Ù†ØµØ¨ Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø§Ø² Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨
pip install git+https://github.com/JustAnotherArchivist/snscrape.git > /dev/null 2>&1

# Ø§Ø¹Ù…Ø§Ù„ Ù¾Ú† Ø¯Ø³ØªÛŒ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø®Ø·Ø§ÛŒ FileFinder
find /usr -path "*snscrape/modules/__init__.py" 2>/dev/null | while read file; do
    echo "ðŸ’‰ Ø§Ø¹Ù…Ø§Ù„ Ù¾Ú† Ø±ÙˆÛŒ $file"
    sudo sed -i 's/importer.find_module(moduleName).load_module(moduleName)/importlib.import_module(f"snscrape.modules.{moduleName}")/g' "$file"
    sudo sed -i '1s/^/import importlib\n/' "$file"
done

# Ù‚Ø¯Ù… 2: Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§
echo "ðŸ”„ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§..."

# Ø§Ø³Ú©Ø±ÛŒÙ¾Øª twitter_scraper.py
cat > scripts/twitter_scraper.py << 'EOF'
import os
import pandas as pd
import snscrape.modules.twitter as sntwitter
from datetime import datetime, timedelta
import sys

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
HASHTAGS = ["#crypto", "#bitcoin", "#ai", "#gaming", "#tech"]
MAX_TWEETS = 200  # Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù‡Ø´ØªÚ¯
DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'data')
os.makedirs(DATA_DIR, exist_ok=True)

def scrape_twitter():
    print("ðŸš€ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§...")
    end_date = datetime.now()
    start_date = end_date - timedelta(days=2)
    all_tweets = []
    
    print(f"ðŸ“Œ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ {len(HASHTAGS)} Ù‡Ø´ØªÚ¯...")
    
    for hashtag in HASHTAGS:
        query = f"{hashtag} since:{start_date.date()} lang:en"
        print(f"ðŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ: {query}")
        
        try:
            tweets_count = 0
            for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):
                if i >= MAX_TWEETS:
                    break
                    
                all_tweets.append([
                    tweet.date,
                    tweet.content,
                    tweet.likeCount,
                    tweet.retweetCount,
                    tweet.replyCount,
                    hashtag
                ])
                tweets_count += 1
            
            print(f"âœ… {tweets_count} ØªÙˆÛŒÛŒØª Ø¨Ø±Ø§ÛŒ {hashtag} Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯")
            
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ {hashtag}: {str(e)}")
    
    if not all_tweets:
        print("âš ï¸ Ù‡ÛŒÚ† ØªÙˆÛŒÛŒØªÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù†Ø´Ø¯! Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        all_tweets = [
            [datetime.now(), "ØªÙˆÛŒÛŒØª Ù†Ù…ÙˆÙ†Ù‡ Û± Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ", 100, 20, 5, "#ai"],
            [datetime.now() - timedelta(hours=1), "ØªÙˆÛŒÛŒØª Ù†Ù…ÙˆÙ†Ù‡ Û² Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø¨ÛŒØªâ€ŒÚ©ÙˆÛŒÙ† Ùˆ Ø§Ø±Ø²Ù‡Ø§ÛŒ Ø¯ÛŒØ¬ÛŒØªØ§Ù„", 50, 10, 2, "#bitcoin"]
        ]
    
    # Ø§ÛŒØ¬Ø§Ø¯ DataFrame
    df = pd.DataFrame(all_tweets, 
                     columns=['datetime', 'content', 'likes', 'retweets', 'replies', 'hashtag'])
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø§Ù…Ù„ Ú©Ù„
    df['engagement'] = df['likes'] + (df['retweets'] * 2) + df['replies']
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡
    output_path = os.path.join(DATA_DIR, "dataset.csv")
    df.to_csv(output_path, index=False)
    print(f"ðŸ’¾ {len(df)} ØªÙˆÛŒÛŒØª Ø¯Ø± {output_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

if __name__ == "__main__":
    scrape_twitter()
EOF

# Ø§Ø³Ú©Ø±ÛŒÙ¾Øª predict.py
cat > scripts/predict.py << 'EOF'
import os
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import numpy as np

# ØªÙ†Ø¸ÛŒÙ… Ù…Ø³ÛŒØ±Ù‡Ø§
DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'data')
MODEL_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'models')
os.makedirs(MODEL_DIR, exist_ok=True)

def train_model():
    print("ðŸ¤– Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ...")
    
    # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    data_path = os.path.join(DATA_DIR, "dataset.csv")
    if not os.path.exists(data_path):
        print(f"âš ï¸ ÙØ§ÛŒÙ„ {data_path} ÛŒØ§ÙØª Ù†Ø´Ø¯!")
        return
        
    df = pd.read_csv(data_path)
    
    # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ
    if len(df) < 2:
        print(f"âš ï¸ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ú©Ø§ÙÛŒ (n_samples={len(df)}). Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø§ Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§")
        X_train = df[['likes', 'retweets', 'replies', 'engagement']]
        y_train = np.random.randint(0, 2, size=len(df))  # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
        
        # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø¯ÙˆÙ† ØªØ³Øª
        model = RandomForestClassifier(n_estimators=100)
        model.fit(X_train, y_train)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
        model_path = os.path.join(MODEL_DIR, "viral_predict_model.pkl")
        joblib.dump(model, model_path)
        print(f"âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¯Ø± {model_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        return
        
    # Ø§ÛŒØ¬Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø¨Ø±Ú†Ø³Ø¨
    X = df[['likes', 'retweets', 'replies', 'engagement']]
    y = np.random.randint(0, 2, size=len(df))  # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
    
    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
    model = RandomForestClassifier(n_estimators=100)
    model.fit(X_train, y_train)
    
    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ðŸ“Š Ø¯Ù‚Øª Ù…Ø¯Ù„: {accuracy:.2f}")
    
    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§
    predictions = model.predict(X)
    df['prediction'] = predictions
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§
    predictions_path = os.path.join(DATA_DIR, "predictions.csv")
    df.to_csv(predictions_path, index=False)
    print(f"ðŸ’¾ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± {predictions_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
    model_path = os.path.join(MODEL_DIR, "viral_predict_model.pkl")
    joblib.dump(model, model_path)
    print(f"ðŸ’¾ Ù…Ø¯Ù„ Ø¯Ø± {model_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

if __name__ == "__main__":
    train_model()
EOF

# Ø§Ø³Ú©Ø±ÛŒÙ¾Øª generate_report.py
cat > scripts/generate_report.py << 'EOF'
import os
import pandas as pd
from datetime import datetime

# ØªÙ†Ø¸ÛŒÙ… Ù…Ø³ÛŒØ±Ù‡Ø§
DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'data')
REPORT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'reports')
os.makedirs(REPORT_DIR, exist_ok=True)

def create_report():
    print("ðŸ“ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ...")
    
    # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    predictions_path = os.path.join(DATA_DIR, "predictions.csv")
    
    if not os.path.exists(predictions_path):
        print(f"âš ï¸ ÙØ§ÛŒÙ„ {predictions_path} ÛŒØ§ÙØª Ù†Ø´Ø¯!")
        return
        
    try:
        df = pd.read_csv(predictions_path)
        
        if df.empty:
            print("âš ï¸ ÙØ§ÛŒÙ„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª!")
            return
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„: {str(e)}")
        return
    
    # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´
    date_str = datetime.now().strftime("%Y-%m-%d_%H-%M")
    report_file = os.path.join(REPORT_DIR, f"viral_report_{date_str}.md")
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# Ú¯Ø²Ø§Ø±Ø´ Ù…Ø­ØªÙˆØ§Ù‡Ø§ÛŒ ÙˆÛŒØ±ÙˆØ³ÛŒ\n\n")
        f.write(f"**ØªØ§Ø±ÛŒØ® ØªÙˆÙ„ÛŒØ¯:** {datetime.now().strftime('%Y/%m/%d %H:%M')}\n\n")
        f.write(f"**ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§:** {len(df)}\n\n")
        
        # Ù…Ø­ØªÙˆØ§Ù‡Ø§ÛŒ Ø¨Ø±ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø§Ù…Ù„
        top_content = df.sort_values('engagement', ascending=False).head(10)
        f.write("## 10 Ù…Ø­ØªÙˆØ§ÛŒ Ø¨Ø±ØªØ± (Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø§Ù…Ù„)\n\n")
        for i, row in top_content.iterrows():
            f.write(f"{i+1}. **{row['content'][:100]}...**\n")
            f.write(f"   - Ù‡Ø´ØªÚ¯: {row['hashtag']}\n")
            f.write(f"   - Ù„Ø§ÛŒÚ©: {row['likes']} | Ø±ÛŒØªÙˆÛŒÛŒØª: {row['retweets']} | Ù¾Ø§Ø³Ø®: {row['replies']}\n")
            f.write(f"   - ØªØ¹Ø§Ù…Ù„ Ú©Ù„: {row['engagement']} | Ø§Ù…ØªÛŒØ§Ø² ÙˆÛŒØ±ÙˆØ³ÛŒ: {row.get('prediction', 0.0)}\n")
            f.write(f"   - ØªØ§Ø±ÛŒØ®: {row['datetime']}\n\n")
    
    print(f"âœ… Ú¯Ø²Ø§Ø±Ø´ Ø¯Ø± {report_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

if __name__ == "__main__":
    create_report()
EOF

# Ù‚Ø¯Ù… 3: Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø§ÛŒÙ†â€ŒÙ„Ø§ÛŒÙ†
echo "ðŸš€ Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§..."
python scripts/twitter_scraper.py

echo "ðŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ..."
python scripts/predict.py

echo "ðŸš€ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ..."
python scripts/generate_report.py

echo "ðŸŽ‰ ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!"
